<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0"
         xsi:schemaLocation="http://docbook.org/ns/docbook http://www.docbook.org/xml/5.0/xsd/docbook.xsd http://www.w3.org/1999/xlink http://www.docbook.org/xml/5.0/xsd/xlink.xsd"
         xml:base="../" xml:id="benchmarkingAndTweaking" xmlns="http://docbook.org/ns/docbook"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xs="http://www.w3.org/2001/XMLSchema"
         xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>Benchmarking and tweaking</title>

  <section>
    <title>Finding the best <literal>Solver</literal> configuration</title>

    <para>OptaPlanner supports several optimization algorithms, but you're probably wondering which is the best one?
    Although some optimization algorithms generally perform better than others, it really depends on your problem
    domain. Most solver phases have parameters which can be tweaked. Those parameters can influence the results a lot,
    even though most solver phases work pretty well out-of-the-box.</para>

    <para>Luckily, OptaPlanner includes a benchmarker, which allows you to play out different solver phases with
    different settings against each other, so you can pick the best configuration for your planning problem.</para>
  </section>

  <section>
    <title>Doing a benchmark</title>

    <section>
      <title>Adding the extra dependency</title>

      <para>The benchmarker is in a separate artifact called <literal>optaplanner-benchmark</literal>.</para>

      <para>If you use Maven, add a dependency in your <filename>pom.xml</filename> file:</para>

      <programlisting language="xml">    &lt;dependency&gt;
      &lt;groupId&gt;org.optaplanner&lt;/groupId&gt;
      &lt;artifactId&gt;optaplanner-benchmark&lt;/artifactId&gt;
      &lt;version&gt;...&lt;/version&gt;
    &lt;/dependency&gt;</programlisting>

      <para>This is similar for Gradle, Ivy and Buildr. The version must be exactly the same as the
      <literal>optaplanner-core</literal> version used.</para>

      <para>If you use ANT, you've probably already copied the required jars from the download zip's
      <filename>binaries</filename> directory.</para>
    </section>

    <section>
      <title>Building and running a <literal>PlannerBenchmark</literal></title>

      <para>You can build a <literal>PlannerBenchmark</literal> instance with the
      <literal>XmlPlannerBenchmarkFactory</literal>. Configure it with a benchmark configuration xml file:</para>

      <programlisting language="java">        PlannerBenchmarkFactory plannerBenchmarkFactory = new XmlPlannerBenchmarkFactory(
                "/org/optaplanner/examples/nqueens/benchmark/nqueensBenchmarkConfig.xml");
        PlannerBenchmark plannerBenchmark = benchmarkFactory.buildPlannerBenchmark();
        plannerBenchmark.benchmark();</programlisting>

      <para>A basic benchmark configuration file looks something like this:</para>

      <programlisting language="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens&lt;/benchmarkDirectory&gt;
  &lt;!--&lt;parallelBenchmarkCount&gt;AUTO&lt;/parallelBenchmarkCount&gt;--&gt;
  &lt;warmUpSecondsSpend&gt;30&lt;/warmUpSecondsSpend&gt;

  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      &lt;xstreamAnnotatedClass&gt;org.optaplanner.examples.nqueens.domain.NQueens&lt;/xstreamAnnotatedClass&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/unsolvedNQueens32.xml&lt;/inputSolutionFile&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/unsolvedNQueens64.xml&lt;/inputSolutionFile&gt;
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;
    &lt;solver&gt;
      &lt;solutionClass&gt;org.optaplanner.examples.nqueens.domain.NQueens&lt;/solutionClass&gt;
      &lt;planningEntityClass&gt;org.optaplanner.examples.nqueens.domain.Queen&lt;/planningEntityClass&gt;
      &lt;scoreDirectorFactory&gt;
        &lt;scoreDefinitionType&gt;SIMPLE&lt;/scoreDefinitionType&gt;
        &lt;scoreDrl&gt;/org/optaplanner/examples/nqueens/solver/nQueensScoreRules.drl&lt;/scoreDrl&gt;
      &lt;/scoreDirectorFactory&gt;
      &lt;termination&gt;
        &lt;maximumSecondsSpend&gt;20&lt;/maximumSecondsSpend&gt;
      &lt;/termination&gt;
      &lt;constructionHeuristic&gt;
        &lt;constructionHeuristicType&gt;FIRST_FIT_DECREASING&lt;/constructionHeuristicType&gt;
        &lt;constructionHeuristicPickEarlyType&gt;FIRST_LAST_STEP_SCORE_EQUAL_OR_IMPROVING&lt;/constructionHeuristicPickEarlyType&gt;
      &lt;/constructionHeuristic&gt;
    &lt;/solver&gt;
  &lt;/inheritedSolverBenchmark&gt;

  &lt;solverBenchmark&gt;
    &lt;name&gt;Entity tabu&lt;/name&gt;
    &lt;solver&gt;
      &lt;localSearch&gt;
        &lt;changeMoveSelector&gt;
          &lt;selectionOrder&gt;ORIGINAL&lt;/selectionOrder&gt;
        &lt;/changeMoveSelector&gt;
        &lt;acceptor&gt;
          &lt;entityTabuSize&gt;5&lt;/entityTabuSize&gt;
        &lt;/acceptor&gt;
        &lt;forager&gt;
          &lt;pickEarlyType&gt;NEVER&lt;/pickEarlyType&gt;
        &lt;/forager&gt;
      &lt;/localSearch&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;Value tabu&lt;/name&gt;
    &lt;solver&gt;
      &lt;localSearch&gt;
        &lt;changeMoveSelector&gt;
          &lt;selectionOrder&gt;ORIGINAL&lt;/selectionOrder&gt;
        &lt;/changeMoveSelector&gt;
        &lt;acceptor&gt;
          &lt;valueTabuSize&gt;5&lt;/valueTabuSize&gt;
        &lt;/acceptor&gt;
        &lt;forager&gt;
          &lt;pickEarlyType&gt;NEVER&lt;/pickEarlyType&gt;
        &lt;/forager&gt;
      &lt;/localSearch&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;Move tabu&lt;/name&gt;
    &lt;solver&gt;
      &lt;localSearch&gt;
        &lt;changeMoveSelector&gt;
          &lt;selectionOrder&gt;ORIGINAL&lt;/selectionOrder&gt;
        &lt;/changeMoveSelector&gt;
        &lt;acceptor&gt;
          &lt;moveTabuSize&gt;5&lt;/moveTabuSize&gt;
        &lt;/acceptor&gt;
        &lt;forager&gt;
          &lt;pickEarlyType&gt;NEVER&lt;/pickEarlyType&gt;
        &lt;/forager&gt;
      &lt;/localSearch&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
&lt;/plannerBenchmark&gt;</programlisting>

      <para>This <literal>PlannerBenchmark</literal> will try 3 configurations (1 move tabu, 1 entity tabu and 1 value
      tabu) on 2 data sets (32 and 64 queens), so it will run 6 solvers.</para>

      <para>Every <literal>solverBenchmark</literal> element contains a solver configuration (for example with a local
      search solver phase) and one or more <literal>inputSolutionFile</literal> elements. It will run the solver
      configuration on each of those unsolved solution files. The element <literal>name</literal> is optional, because
      it is generated if absent. The inputSolutionFile is read by a <link linkend="problemIO">ProblemIO</link>.</para>

      <para>To lower verbosity, the common part of multiple <literal>solverBenchmark</literal> entities can be extracted
      to the <literal>inheritedSolverBenchmark</literal> element. Yet, every element can still be overwritten per
      <literal>solverBenchmark</literal> element. Note that inherited solver phases such as
      <literal>&lt;constructionHeuristic&gt;</literal> or <literal>&lt;localSearch&gt;</literal> are not overwritten but
      instead are added to the tail of the solver phases list.</para>

      <para>You need to specify a <literal>benchmarkDirectory</literal> (relative to the working directory). A benchmark
      report will be written in that directory.</para>

      <note>
        <para>It's recommended that the <literal>benchmarkDirectory</literal> is a directory ignored for source control
        and not cleaned by your build system. This way the generated files are not bloating your source control and they
        aren't lost when doing a build. Usually that directory is called <literal>local</literal>.</para>
      </note>
    </section>

    <section xml:id="problemIO">
      <title>ProblemIO: input and output of Solution files</title>

      <section>
        <title><literal>ProblemIO</literal> interface</title>

        <para>The benchmarker needs to be able to read the input files to contain a <literal>Solution</literal> write
        the best <literal>Solution</literal> of each benchmark to an output file. For that it uses a class that
        implements the <literal>ProblemIO</literal> interface:</para>

        <programlisting language="java">public interface ProblemIO {

    String getFileExtension();

    Solution read(File inputSolutionFile);

    void write(Solution solution, File outputSolutionFile);

}</programlisting>

        <warning>
          <para>Your input files need to have been written with the same <literal>ProblemIO</literal> class as they are
          being read by the benchmarker.</para>
        </warning>
      </section>

      <section>
        <title><literal>XStreamProblemIO</literal>: the default <literal>ProblemIO</literal></title>

        <para>By default, a benchmarker uses a <literal>XStreamProblemIO</literal> instance to read and write
        solutions.</para>

        <para>You need to tell the benchmarker about your <literal>Solution</literal> class which is annotated with
        XStream annotations:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      &lt;xstreamAnnotatedClass&gt;org.optaplanner.examples.nqueens.domain.NQueens&lt;/xstreamAnnotatedClass&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/unsolvedNQueens32.xml&lt;/inputSolutionFile&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>

        <para>Your input files need to have been written with a <literal>XStreamProblemIO</literal> instance, not just
        any <literal>XStream</literal> instance, because the <literal>XStreamProblemIO</literal> uses a customized
        <literal>XStream</literal> instance.</para>

        <warning>
          <para>XStream (and XML in general) is a very verbose format. Reading or writing large datasets in this format
          can cause an <literal>OutOfMemoryError</literal> and performance degradation.</para>
        </warning>
      </section>

      <section>
        <title>Custom <literal>ProblemIO</literal></title>

        <para>Alternatively, you can implement your own <literal>ProblemIO</literal> implementation and configure it
        with the <literal>problemIOClass</literal> element:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      &lt;problemIOClass&gt;org.optaplanner.examples.machinereassignment.persistence.MachineReassignmentProblemIO&lt;/problemIOClass&gt;
      &lt;inputSolutionFile&gt;data/machinereassignment/input/model_a1_1.txt&lt;/inputSolutionFile&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>

        <warning>
          <para>A <literal>ProblemIO</literal> implementation must be thread-safe.</para>
        </warning>
      </section>
    </section>

    <section xml:id="writingTheOutputSolutionOfTheBenchmarkRuns">
      <title>Writing the output solution of the benchmark runs</title>

      <para>The best solution of each benchmark run can be written to the in the <literal>benchmarkDirectory</literal>.
      By default, this is disabled, because the files are rarely used and considered bloat. Also, on large datasets,
      writing the best solution of each single benchmark can take quite some time and memory (causing an
      <literal>OutOfMemoryError</literal>), especially in a verbose format like XStream.</para>

      <para>You can enable to write the output solution in the <literal>benchmarkDirectory</literal> with
      <literal>writeOutputSolutionEnabled</literal>:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;writeOutputSolutionEnabled&gt;true&lt;/writeOutputSolutionEnabled&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>
    </section>

    <section>
      <title>Warming up the HotSpot compiler</title>

      <para><emphasis role="bold">Without a warm up, the results of the first (or first few) benchmarks are not
      reliable</emphasis>, because they will have lost CPU time on HotSpot JIT compilation (and possibly DRL compilation
      too).</para>

      <para>The avoid that distortion, the benchmarker can run some of the benchmarks for a specified amount of time,
      before running the real benchmarks. Generally, a warm up of 30 seconds suffices:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;warmUpSecondsSpend&gt;30&lt;/warmUpSecondsSpend&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>
    </section>
  </section>

  <section>
    <title>Benchmark report</title>

    <section>
      <title>HTML report</title>

      <para>After the running a benchmark, a HTML report will be written in the <literal>benchmarkDirectory</literal>
      with the filename <filename>index.html</filename>. Open it in your browser. It has a nice overview of your
      benchmark including:</para>

      <itemizedlist>
        <listitem>
          <para>Summary statistics: graphs and tables</para>
        </listitem>

        <listitem>
          <para>Problem statistics per <literal>inputSolutionFile</literal></para>
        </listitem>

        <listitem>
          <para>Each solver configuration (ranked): easy to copy and paste.</para>
        </listitem>

        <listitem>
          <para>Benchmark information</para>
        </listitem>
      </itemizedlist>

      <para>The HTML report will use your default locale to format numbers. If you need to share the benchmark report
      with people from another country, you might want to overwrite the <literal>benchmarkReportLocale</literal>:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;benchmarkReportLocale&gt;en_US&lt;/benchmarkReportLocale&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>
    </section>

    <section>
      <title>Summary statistics</title>

      <section>
        <title>Best score summary (graph and table)</title>

        <para>Shows the best score per <literal>inputSolutionFile</literal> for each solver configuration.</para>

        <figure>
          <title>Best score summary statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/summaryStatistic.png" format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Best score scalability summary (graph and table)</title>

        <para>Shows the best score per problem scale for each solver configuration.</para>
      </section>

      <section>
        <title>Winning score difference summary (graph and table)</title>

        <para>Shows the winning score difference score per <literal>inputSolutionFile</literal> for each solver
        configuration. The winning score difference is the score difference with the score of the winning solver
        configuration for that particular <literal>inputSolutionFile</literal>.</para>
      </section>

      <section>
        <title>Worst score difference percentage (ROI) summary (graph and table)</title>

        <para>Shows the return on investment (ROI) per <literal>inputSolutionFile</literal> for each solver
        configuration if you'd upgrade from the worst solver configuration for that particular
        <literal>inputSolutionFile</literal>.</para>
      </section>

      <section>
        <title>Time spend summary (graph and table)</title>

        <para>Shows the time spend per <literal>inputSolutionFile</literal> for each solver configuration. This is not
        useful if it's benchmarking against a fixed time limit.</para>
      </section>

      <section>
        <title>Time spend scalability summary (graph and table)</title>

        <para>Shows the time spend per problem scale for each solver configuration. This is not useful if it's
        benchmarking against a fixed time limit.</para>
      </section>

      <section>
        <title>Average calculation count summary (graph and table)</title>

        <para>Shows the score calculation speed: the average calculation count per second per problem scale for each
        solver configuration.</para>

        <para>This is very useful to compare the performance and scalability of different score calculator and/or score
        rule implementations.</para>
      </section>
    </section>

    <section>
      <title>Statistic per data set (graph and CSV)</title>

      <section>
        <title>Enabling a problem statistic</title>

        <para>The benchmarker supports outputting problem statistics as graphs and CSV (comma separated values) files to
        the <literal>benchmarkDirectory</literal>.</para>

        <para>To configure graph and CSV output of a statistic, just add a <literal>problemStatisticType</literal>
        line:</para>

        <programlisting language="xml">&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens/solved&lt;/benchmarkDirectory&gt;
  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
      &lt;problemStatisticType&gt;CALCULATE_COUNT_PER_SECOND&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;
    ...
  &lt;/inheritedSolverBenchmark&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

        <para>Multiple <literal>problemStatisticType</literal> elements are allowed. Some statistic types might
        influence performance and benchmark results noticeably. The following types are supported:</para>
      </section>

      <section>
        <title>Best score over time statistic (graph and CSV)</title>

        <para>To see how the best score evolves over time, add:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

        <figure>
          <title>Best score over time statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestScoreStatistic.png" format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para><emphasis role="bold">The best score over time statistic is very useful to detect abnormalities, such as a
        potential <link linkend="scoreTrap">score trap</link>.</emphasis></para>

        <note>
          <para>A time gradient based algorithm (such as Simulated Annealing) will have a different statistic if it's
          run with a different time limit configuration. That's because this Simulated Annealing implementation
          automatically determines its velocity based on the amount of time that can be spend. On the other hand, for
          the Tabu Search and Late Annealing, what you see is what you'd get.</para>
        </note>
      </section>

      <section>
        <title>Step score over time statistic (graph and CSV)</title>

        <para>To see how the step score evolves over time, add:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;STEP_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

        <figure>
          <title>Step score over time statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/stepScoreStatistic.png" format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Compare the step score statistic with the best score statistic (especially on parts for which the best
        score flatlines). If it hits a local optima, the solver should take deteriorating steps to escape it. But it
        shouldn't deteriorate too much either.</para>
      </section>

      <section>
        <title>Calculate count per second statistic (graph and CSV)</title>

        <para>To see how fast the scores are calculated, add:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;CALCULATE_COUNT_PER_SECOND&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

        <figure>
          <title>Calculate count per second statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/calculateCountPerSecondStatistic.png"
                         format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <note>
          <para>The initial high calculate count is typical during solution initialization: it's far easier to calculate
          the score of a solution if only a handful planning entities have been initialized, than when all the planning
          entities are initialized.</para>

          <para>After those few seconds of initialization, the calculate count is relatively stable, apart from an
          occasional stop-the-world garbage collector disruption.</para>
        </note>
      </section>

      <section>
        <title>Best solution mutation over time statistic (graph and CSV)</title>

        <para>To see how much each new best solution differs from the <emphasis>previous best solution</emphasis>, by
        counting the number of planning variables which have a different value (not including the variables that have
        changed multiple times but still end up with the same value), add:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SOLUTION_MUTATION&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

        <figure>
          <title>Best solution mutation over time statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestSolutionMutationStatistic.png"
                         format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Use Tabu Search - an algorithm that behaves like a human - to get an estimation on how difficult it would
        be for a human to improve the previous best solution to that new best solution.</para>
      </section>

      <section>
        <title>Memory use statistic (graph and CSV)</title>

        <para>To see how much memory is used, add:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;MEMORY_USE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

        <figure>
          <title>Memory use statistic</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/memoryUseStatistic.png" format="PNG"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Ranking the <literal>Solver</literal>s</title>

      <para>The benchmark report automatically ranks the solvers. The <literal>Solver</literal> with rank
      <literal>0</literal> is called the favorite <literal>Solver</literal>: it performs best overall, but it might not
      be the best on every problem. It's recommended to use that favorite <literal>Solver</literal> in
      production.</para>

      <para>However, there are different ways of ranking the solvers. You can configure how:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;solverBenchmarkRankingType&gt;TOTAL_SCORE&lt;/solverBenchmarkRankingType&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <para>The following <literal>solverBenchmarkRankingType</literal>s are supported:</para>

      <itemizedlist>
        <listitem>
          <para><literal>TOTAL_SCORE</literal> (default): Maximize the overall score, so minimize the overall cost if
          all solutions would be executed.</para>
        </listitem>

        <listitem>
          <para><literal>WORST_SCORE</literal>: Minimize the worst case scenario.</para>
        </listitem>

        <listitem>
          <para><literal>TOTAL_RANKING</literal>: Maximize the overall ranking. Use this if your datasets differ greatly
          in size or difficulty, producing a difference in <literal>Score</literal> magnitude.</para>
        </listitem>
      </itemizedlist>

      <para>You can also use a custom ranking, by implementing a <literal>Comparator</literal>:</para>

      <programlisting language="xml">  &lt;solverBenchmarkRankingComparatorClass&gt;...TotalScoreSolverBenchmarkRankingComparator&lt;/solverBenchmarkRankingComparatorClass&gt;</programlisting>

      <para>Or a weight factory:</para>

      <programlisting language="xml">  &lt;solverBenchmarkRankingWeightFactoryClass&gt;...TotalRankSolverBenchmarkRankingWeightFactory&lt;/solverBenchmarkRankingWeightFactoryClass&gt;</programlisting>
    </section>
  </section>

  <section>
    <title>Advanced benchmarking</title>

    <section>
      <title>Benchmarking performance tricks</title>

      <section>
        <title>Parallel benchmarking on multiple threads</title>

        <para>If you have multiple processors available on your computer, you can run multiple benchmarks in parallel on
        multiple threads to get your benchmarks results faster:</para>

        <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;parallelBenchmarkCount&gt;AUTO&lt;/parallelBenchmarkCount&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

        <warning>
          <para>Running too many benchmarks in parallel will affect the results of benchmarks negatively. Leave some
          processors unused for garbage collection and other processes.</para>

          <para>We tweak <literal>parallelBenchmarkCount</literal> <literal>AUTO</literal> to maximize the reliability
          and efficiency of the benchmark results.</para>
        </warning>

        <para>The following <literal>parallelBenchmarkCount</literal>s are supported:</para>

        <itemizedlist>
          <listitem>
            <para><literal>1</literal> (default): Run all benchmarks sequentially.</para>
          </listitem>

          <listitem>
            <para><literal>AUTO</literal>: Let Planner decide how many benchmarks to run in parallel. This formula is
            based on experience. It's recommended to prefer this over the other parallel enabling options.</para>
          </listitem>

          <listitem>
            <para>Static number: The number of benchmarks to run in parallel.</para>

            <programlisting language="xml">&lt;parallelBenchmarkCount&gt;2&lt;/parallelBenchmarkCount&gt;</programlisting>
          </listitem>

          <listitem>
            <para>JavaScript formula: Formula for the number of benchmarks to run in parallel. It can use the variable
            <literal>availableProcessorCount</literal>. For example:</para>

            <programlisting language="xml">&lt;parallelBenchmarkCount&gt;(availableProcessorCount / 2) + 1&lt;/parallelBenchmarkCount&gt;</programlisting>
          </listitem>
        </itemizedlist>

        <note>
          <para>The <literal>parallelBenchmarkCount</literal> is always limited to the number of available processors.
          If it's higher, it will be automatically decreased.</para>
        </note>

        <note>
          <para>In the future, we will also support multi-JVM benchmarking. This feature is independent of <link
          xlink:href="https://issues.jboss.org/browse/PLANNER-76">multi-threaded solving</link> or multi-JVM
          solving.</para>
        </note>
      </section>
    </section>

    <section>
      <title>Template based benchmarking and matrix benchmarking</title>

      <para>Matrix benchmarking is benchmarking a combination of value sets. For example: benchmark 4
      <literal>entityTabuSize</literal> values (<literal>5</literal>, <literal>7</literal>, <literal>11</literal> and
      <literal>13</literal>) combined with 3 <literal>acceptedCountLimit</literal> values (<literal>500</literal>,
      <literal>1000</literal> and <literal>2000</literal>), resulting in 12 solver configurations.</para>

      <para>To reduce the verbosity of such a benchmark configuration, you can use a <link
      xlink:href="http://freemarker.sourceforge.net/">Freemarker</link> template for the benchmark configuration
      instead:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...

  &lt;inheritedSolverBenchmark&gt;
    ...
  &lt;/inheritedSolverBenchmark&gt;

&lt;#list [5, 7, 11, 13] as entityTabuSize&gt;
&lt;#list [500, 1000, 2000] as acceptedCountLimit&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;entityTabuSize ${entityTabuSize} acceptedCountLimit ${acceptedCountLimit}&lt;/name&gt;
    &lt;solver&gt;
      &lt;localSearch&gt;
        &lt;unionMoveSelector&gt;
          &lt;changeMoveSelector/&gt;
          &lt;swapMoveSelector/&gt;
        &lt;/unionMoveSelector&gt;
        &lt;acceptor&gt;
          &lt;entityTabuSize&gt;${entityTabuSize}&lt;/entityTabuSize&gt;
        &lt;/acceptor&gt;
        &lt;forager&gt;
          &lt;acceptedCountLimit&gt;${acceptedCountLimit}&lt;/acceptedCountLimit&gt;
        &lt;/forager&gt;
      &lt;/localSearch&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
&lt;/#list&gt;
&lt;/#list&gt;
&lt;/plannerBenchmark&gt;</programlisting>

      <para>And build it with the class <literal>FreemarkerXmlPlannerBenchmarkFactory</literal>:</para>

      <programlisting language="java">        PlannerBenchmarkFactory plannerBenchmarkFactory = new FreemarkerXmlPlannerBenchmarkFactory(
                "/org/optaplanner/examples/cloudbalancing/benchmark/cloudBalancingBenchmarkConfigTemplate.xml.ftl");
        PlannerBenchmark plannerBenchmark = benchmarkFactory.buildPlannerBenchmark();</programlisting>
    </section>
  </section>
</chapter>
