<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0"
         xsi:schemaLocation="http://docbook.org/ns/docbook http://www.docbook.org/xml/5.0/xsd/docbook.xsd http://www.w3.org/1999/xlink http://www.docbook.org/xml/5.0/xsd/xlink.xsd"
         xml:base="../" xml:id="benchmarker" xmlns="http://docbook.org/ns/docbook"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xs="http://www.w3.org/2001/XMLSchema"
         xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>Benchmarking And Tweaking</title>

  <section xml:id="findTheBestSolverConfiguration">
    <title>Find The Best <literal>Solver</literal> Configuration</title>

    <para>Planner supports several optimization algorithms, so you're probably wondering which is the best one? Although
    some optimization algorithms generally perform better than others, it really depends on your problem domain. Most
    solver phases have parameters which can be tweaked. Those parameters can influence the results a lot, even though
    most solver phases work pretty well out-of-the-box.</para>

    <para>Luckily, Planner includes a benchmarker, which allows you to play out different solver phases with different
    settings against each other in development, so you can use the best configuration for your planning problem in
    production.</para>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/benchmarkOverview.png"/>
      </imageobject>
    </mediaobject>
  </section>

  <section xml:id="benchmarkConfiguration">
    <title>Benchmark Configuration</title>

    <section xml:id="benchmarker.addDependency">
      <title>Add Dependency On <literal>optaplanner-benchmark</literal></title>

      <para>The benchmarker is in a separate artifact called <literal>optaplanner-benchmark</literal>.</para>

      <para>If you use Maven, add a dependency in your <filename>pom.xml</filename> file:</para>

      <programlisting language="xml">    &lt;dependency&gt;
      &lt;groupId&gt;org.optaplanner&lt;/groupId&gt;
      &lt;artifactId&gt;optaplanner-benchmark&lt;/artifactId&gt;
    &lt;/dependency&gt;</programlisting>

      <para>This is similar for Gradle, Ivy and Buildr. The version must be exactly the same as the
      <literal>optaplanner-core</literal> version used (which is automatically the case if you import
      <literal>optaplanner-bom</literal>).</para>

      <para>If you use ANT, you've probably already copied the required jars from the download zip's
      <filename>binaries</filename> directory.</para>
    </section>

    <section xml:id="buildAndRunAPlannerBenchmark">
      <title>Build And Run A <literal>PlannerBenchmark</literal></title>

      <para>Build a <literal>PlannerBenchmark</literal> instance with a <literal>PlannerBenchmarkFactory</literal>.
      Configure it with a benchmark configuration XML file, provided as a classpath resource:</para>

      <programlisting language="java">        PlannerBenchmarkFactory plannerBenchmarkFactory = PlannerBenchmarkFactory.createFromXmlResource(
                "org/optaplanner/examples/nqueens/benchmark/nqueensBenchmarkConfig.xml");
        PlannerBenchmark plannerBenchmark = plannerBenchmarkFactory.buildPlannerBenchmark();
        plannerBenchmark.benchmark();</programlisting>

      <para>A benchmark configuration file looks like this:</para>

      <programlisting language="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens&lt;/benchmarkDirectory&gt;

  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      ...
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/32queens.xml&lt;/inputSolutionFile&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/64queens.xml&lt;/inputSolutionFile&gt;
    &lt;/problemBenchmarks&gt;
    &lt;solver&gt;
      ...&lt;!-- Common solver configuration --&gt;
    &lt;/solver&gt;
  &lt;/inheritedSolverBenchmark&gt;

  &lt;solverBenchmark&gt;
    &lt;name&gt;Tabu Search&lt;/name&gt;
    &lt;solver&gt;
      ...&lt;!-- Tabu Search specific solver configuration --&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;Simulated Annealing&lt;/name&gt;
    &lt;solver&gt;
      ...&lt;!-- Simulated Annealing specific solver configuration --&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;Late Acceptance&lt;/name&gt;
    &lt;solver&gt;
      ...&lt;!-- Late Acceptance specific solver configuration --&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
&lt;/plannerBenchmark&gt;</programlisting>

      <para>This <literal>PlannerBenchmark</literal> will try 3 configurations (Tabu Search, Simulated Annealing and
      Late Acceptance) on 2 data sets (32queens and 64queens), so it will run 6 solvers.</para>

      <para>Every <literal>&lt;solverBenchmark&gt;</literal> element contains a solver configuration and one or more
      <literal>&lt;inputSolutionFile&gt;</literal> elements. It will run the solver configuration on each of those
      unsolved solution files. The element <literal>name</literal> is optional, because it is generated if absent. The
      <literal>inputSolutionFile</literal> is read by a <link linkend="solutionFileIO">SolutionFileIO</link> (relative
      to the working directory).</para>

      <note>
        <para>Use a forward slash (<literal>/</literal>) as the file separator (for example in the element
        <literal>&lt;inputSolutionFile&gt;</literal>). That will work on any platform (including Windows).</para>

        <para>Do not use backslash (<literal>\</literal>) as the file separator: that breaks portability because it does
        not work on Linux and Mac.</para>
      </note>

      <para>The benchmark report will be written in the directory specified the
      <literal>&lt;benchmarkDirectory&gt;</literal> element (relative to the working directory).</para>

      <note>
        <para>It's recommended that the <literal>benchmarkDirectory</literal> is a directory ignored for source control
        and not cleaned by your build system. This way the generated files are not bloating your source control and they
        aren't lost when doing a build. Usually that directory is called <literal>local</literal>.</para>
      </note>

      <para>If an <literal>Exception</literal> or <literal>Error</literal> occurs in a single benchmark, the entire
      Benchmarker will not fail-fast (unlike everything else in Planner). Instead, the Benchmarker will continue to run
      all other benchmarks, write the benchmark report and then fail (if there is at least 1 failing single benchmark).
      The failing benchmarks will be clearly marked as such in the benchmark report.</para>

      <section xml:id="inheritedSolverBenchmark">
        <title>Inherited solver benchmark</title>

        <para>To lower verbosity, the common parts of multiple <literal>&lt;solverBenchmark&gt;</literal> elements are
        extracted to the <literal>&lt;inheritedSolverBenchmark&gt;</literal> element. Every property can still be
        overwritten per <literal>&lt;solverBenchmark&gt;</literal> element. Note that inherited solver phases such as
        <literal>&lt;constructionHeuristic&gt;</literal> or <literal>&lt;localSearch&gt;</literal> are not overwritten
        but instead are added to the tail of the solver phases list.</para>
      </section>
    </section>

    <section xml:id="solutionFileIO">
      <title>SolutionFileIO: Input And Output Of Solution Files</title>

      <section xml:id="solutionFileIOInterface">
        <title><literal>SolutionFileIO</literal> Interface</title>

        <para>The benchmarker needs to be able to read the input files to load a <literal>Solution</literal>. Also, it
        might need to write the best <literal>Solution</literal> of each benchmark to an output file. For that it uses a
        class that implements the <literal>SolutionFileIO</literal> interface:</para>

        <programlisting language="java">public interface SolutionFileIO {

    String getInputFileExtension();

    String getOutputFileExtension();

    Solution read(File inputSolutionFile);

    void write(Solution solution, File outputSolutionFile);

}</programlisting>

        <para>The <literal>SolutionFileIO</literal> interface is in the
        <literal>optaplanner-persistence-common</literal> jar (which is a dependency of the
        <literal>optaplanner-benchmark</literal> jar).</para>
      </section>

      <section xml:id="xStreamSolutionFileIO">
        <title><literal>XStreamSolutionFileIO</literal>: The Default <literal>SolutionFileIO</literal></title>

        <para>By default, a benchmarker uses a <literal>XStreamSolutionFileIO</literal> instance to read and write
        solutions.</para>

        <para>It's required to tell the benchmarker about your <literal>Solution</literal> class which is annotated with
        XStream annotations:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      &lt;xStreamAnnotatedClass&gt;org.optaplanner.examples.nqueens.domain.NQueens&lt;/xStreamAnnotatedClass&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/32queens.xml&lt;/inputSolutionFile&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>

        <para>Those input files need to have been written with a <literal>XStreamSolutionFileIO</literal> instance, not
        just any <literal>XStream</literal> instance, because the <literal>XStreamSolutionFileIO</literal> uses a
        customized <literal>XStream</literal> instance.</para>

        <warning>
          <para>XStream (and XML in general) is a very verbose format. Reading or writing very large datasets in this
          format can cause an <literal>OutOfMemoryError</literal> and performance degradation.</para>
        </warning>
      </section>

      <section xml:id="customSolutionFileIO">
        <title>Custom <literal>SolutionFileIO</literal></title>

        <para>Alternatively, implement your own <literal>SolutionFileIO</literal> implementation and configure it with
        the <literal>solutionFileIOClass</literal> element:</para>

        <programlisting language="xml">    &lt;problemBenchmarks&gt;
      &lt;solutionFileIOClass&gt;org.optaplanner.examples.machinereassignment.persistence.MachineReassignmentFileIO&lt;/solutionFileIOClass&gt;
      &lt;inputSolutionFile&gt;data/machinereassignment/import/model_a1_1.txt&lt;/inputSolutionFile&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>

        <para>It's recommended that output files can be read as input files, which also implies that
        <literal>getInputFileExtension()</literal> and <literal>getOutputFileExtension()</literal> return the same
        value.</para>

        <warning>
          <para>A <literal>SolutionFileIO</literal> implementation must be thread-safe.</para>
        </warning>
      </section>

      <section xml:id="readingAnInputSolutionFromADatabase">
        <title>Reading An Input Solution From A Database (Or Other Repository)</title>

        <para>The benchmark configuration currently expects an <literal>&lt;inputSolutionFile&gt;</literal> element for
        each dataset. There are 2 ways to deal with this if your dataset is in a database or another type of
        repository:</para>

        <itemizedlist>
          <listitem>
            <para>Extract the datasets from the database and serialize them to a local file (for example as XML with
            <literal>XStreamSolutionFileIO</literal>). Then use those files an
            <literal>&lt;inputSolutionFile&gt;</literal> elements.</para>
          </listitem>

          <listitem>
            <para>For each dataset, create a txt file that holds the unique id of the dataset. Write <link
            linkend="customSolutionFileIO">a custom <literal>SolutionFileIO</literal></link> that reads that identifier,
            connects to the database and extract the problem identified by that id. Configure those txt files as
            <literal>&lt;inputSolutionFile&gt;</literal> elements.</para>
          </listitem>
        </itemizedlist>

        <note>
          <para>Local files are always faster and don't require a network connection.</para>
        </note>
      </section>
    </section>

    <section xml:id="warmingUpTheHotSpotCompiler">
      <title>Warming Up The HotSpot Compiler</title>

      <para><emphasis role="bold">Without a warm up, the results of the first (or first few) benchmarks are not
      reliable</emphasis>, because they will have lost CPU time on HotSpot JIT compilation (and possibly DRL compilation
      too).</para>

      <para>To avoid that distortion, the benchmarker can run some of the benchmarks for a specified amount of time,
      before running the real benchmarks. Generally, a warm up of 30 seconds suffices:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;warmUpSecondsSpentLimit&gt;30&lt;/warmUpSecondsSpentLimit&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <note>
        <para>The warm up time budget does not include the time it takes to load the datasets. With large datasets, this
        can cause the warm up to run considerably longer than specified in the configuration.</para>
      </note>
    </section>

    <section xml:id="benchmarkBlueprint">
      <title>Benchmark Blueprint: A Predefined Configuration</title>

      <para>To quickly configure and run a benchmark for typical solver configs, use a
      <literal>solverBenchmarkBluePrint</literal> instead of <literal>solverBenchmark</literal>s:</para>

      <programlisting language="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens&lt;/benchmarkDirectory&gt;
  &lt;warmUpSecondsSpentLimit&gt;30&lt;/warmUpSecondsSpentLimit&gt;

  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      &lt;xStreamAnnotatedClass&gt;org.optaplanner.examples.nqueens.domain.NQueens&lt;/xStreamAnnotatedClass&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/32queens.xml&lt;/inputSolutionFile&gt;
      &lt;inputSolutionFile&gt;data/nqueens/unsolved/64queens.xml&lt;/inputSolutionFile&gt;
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;
    &lt;solver&gt;
      &lt;scanAnnotatedClasses/&gt;
      &lt;scoreDirectorFactory&gt;
        &lt;scoreDefinitionType&gt;SIMPLE&lt;/scoreDefinitionType&gt;
        &lt;scoreDrl&gt;org/optaplanner/examples/nqueens/solver/nQueensScoreRules.drl&lt;/scoreDrl&gt;
        &lt;initializingScoreTrend&gt;ONLY_DOWN&lt;/initializingScoreTrend&gt;
      &lt;/scoreDirectorFactory&gt;
      &lt;termination&gt;
        &lt;minutesSpentLimit&gt;1&lt;/minutesSpentLimit&gt;
      &lt;/termination&gt;
    &lt;/solver&gt;
  &lt;/inheritedSolverBenchmark&gt;

  &lt;solverBenchmarkBluePrint&gt;
    &lt;solverBenchmarkBluePrintType&gt;EVERY_CONSTRUCTION_HEURISTIC_TYPE_WITH_EVERY_LOCAL_SEARCH_TYPE&lt;/solverBenchmarkBluePrintType&gt;
  &lt;/solverBenchmarkBluePrint&gt;
&lt;/plannerBenchmark&gt;</programlisting>

      <para>The following <literal>SolverBenchmarkBluePrintType</literal>s are supported:</para>

      <itemizedlist>
        <listitem>
          <para><literal>EVERY_CONSTRUCTION_HEURISTIC_TYPE</literal>: Run every Construction Heuristic type (First Fit,
          First Fit Decreasing, Cheapest Insertion, ...).</para>
        </listitem>
      </itemizedlist>

      <itemizedlist>
        <listitem>
          <para><literal>EVERY_LOCAL_SEARCH_TYPE</literal>: Run every Local Search type (Tabu Search, Late Acceptance,
          ...) with the default Construction Heuristic.</para>
        </listitem>
      </itemizedlist>

      <itemizedlist>
        <listitem>
          <para><literal>EVERY_CONSTRUCTION_HEURISTIC_TYPE_WITH_EVERY_LOCAL_SEARCH_TYPE</literal>: Run every
          Construction Heuristic type with every Local Search type.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section xml:id="writeTheOutputSolutionOfBenchmarkRuns">
      <title>Write The Output Solution Of Benchmark Runs</title>

      <para>The best solution of each benchmark run can be written in the <literal>benchmarkDirectory</literal>. By
      default, this is disabled, because the files are rarely used and considered bloat. Also, on large datasets,
      writing the best solution of each single benchmark can take quite some time and memory (causing an
      <literal>OutOfMemoryError</literal>), especially in a verbose format like XStream XML.</para>

      <para>To write those solutions in the <literal>benchmarkDirectory</literal>, enable
      <literal>writeOutputSolutionEnabled</literal>:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;writeOutputSolutionEnabled&gt;true&lt;/writeOutputSolutionEnabled&gt;
      ...
    &lt;/problemBenchmarks&gt;</programlisting>
    </section>

    <section xml:id="benchmarkLogging">
      <title>Benchmark Logging</title>

      <para>Benchmark logging is configured like <link linkend="logging">the <literal>Solver</literal>
      logging</link>.</para>

      <para>To separate the log messages of each single benchmark run into a separate file, use the <link
      xlink:href="http://logback.qos.ch/manual/mdc.html">MDC</link> with key <literal>singleBenchmark.name</literal> in
      a sifting appender. For example with Logback in <literal>logback.xml</literal>:</para>

      <programlisting language="xml">  &lt;appender name="fileAppender" class="ch.qos.logback.classic.sift.SiftingAppender"&gt;
    &lt;discriminator&gt;
      &lt;key&gt;singleBenchmark.name&lt;/key&gt;
      &lt;defaultValue&gt;app&lt;/defaultValue&gt;
    &lt;/discriminator&gt;
    &lt;sift&gt;
      &lt;appender name="fileAppender.${singleBenchmark.name}" class="...FileAppender"&gt;
        &lt;file&gt;local/log/optaplannerBenchmark-${singleBenchmark.name}.log&lt;/file&gt;
        ...
      &lt;/appender&gt;
    &lt;/sift&gt;
  &lt;/appender&gt;</programlisting>
    </section>
  </section>

  <section xml:id="benchmarkReport">
    <title>Benchmark Report</title>

    <section xml:id="benchmarkHtmlReport">
      <title>HTML Report</title>

      <para>After running a benchmark, an HTML report will be written in the <literal>benchmarkDirectory</literal> with
      the <filename>index.html</filename> filename. Open it in your browser. It has a nice overview of your benchmark
      including:</para>

      <itemizedlist>
        <listitem>
          <para>Summary statistics: graphs and tables</para>
        </listitem>

        <listitem>
          <para>Problem statistics per <literal>inputSolutionFile</literal>: graphs and CSV</para>
        </listitem>

        <listitem>
          <para>Each solver configuration (ranked): Handy to copy and paste</para>
        </listitem>

        <listitem>
          <para>Benchmark information: settings, hardware, ...</para>
        </listitem>
      </itemizedlist>

      <note>
        <para>Graphs are generated by the excellent <link
        xlink:href="http://www.jfree.org/jfreechart/">JFreeChart</link> library.</para>
      </note>

      <para>The HTML report will use your default locale to format numbers. If you share the benchmark report with
      people from another country, consider overwriting the <literal>locale</literal> accordingly:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;benchmarkReport&gt;
    &lt;locale&gt;en_US&lt;/locale&gt;
  &lt;/benchmarkReport&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>
    </section>

    <section xml:id="rankingTheSolvers">
      <title>Ranking The <literal>Solver</literal>s</title>

      <para>The benchmark report automatically ranks the solvers. The <literal>Solver</literal> with rank
      <literal>0</literal> is called the favorite <literal>Solver</literal>: it performs best overall, but it might not
      be the best on every problem. It's recommended to use that favorite <literal>Solver</literal> in
      production.</para>

      <para>However, there are different ways of ranking the solvers. Configure it like this:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;benchmarkReport&gt;
    &lt;solverRankingType&gt;TOTAL_SCORE&lt;/solverRankingType&gt;
  &lt;/benchmarkReport&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <para>The following <literal>solverRankingType</literal>s are supported:</para>

      <itemizedlist>
        <listitem>
          <para><literal>TOTAL_SCORE</literal> (default): Maximize the overall score, so minimize the overall cost if
          all solutions would be executed.</para>
        </listitem>

        <listitem>
          <para><literal>WORST_SCORE</literal>: Minimize the worst case scenario.</para>
        </listitem>

        <listitem>
          <para><literal>TOTAL_RANKING</literal>: Maximize the overall ranking. Use this if your datasets differ greatly
          in size or difficulty, producing a difference in <literal>Score</literal> magnitude.</para>
        </listitem>
      </itemizedlist>

      <para><literal>Solver</literal>s with at least one failed single benchmark do not get a ranking.
      <literal>Solver</literal>s with not fully initialized solutions are ranked worse.</para>

      <para>You can also use a custom ranking, by implementing a <literal>Comparator</literal>:</para>

      <programlisting language="xml">  &lt;benchmarkReport&gt;
    &lt;solverRankingComparatorClass&gt;...TotalScoreSolverRankingComparator&lt;/solverRankingComparatorClass&gt;
  &lt;/benchmarkReport&gt;</programlisting>

      <para>Or by implementing a weight factory:</para>

      <programlisting language="xml">  &lt;benchmarkReport&gt;
    &lt;solverRankingWeightFactoryClass&gt;...TotalRankSolverRankingWeightFactory&lt;/solverRankingWeightFactoryClass&gt;
  &lt;/benchmarkReport&gt;</programlisting>
    </section>
  </section>

  <section xml:id="benchmarkReportSummaryStatistics">
    <title>Summary Statistics</title>

    <section xml:id="benchmarkReportBestScoreSummary">
      <title>Best Score Summary (Graph And Table)</title>

      <para>Shows the best score per <literal>inputSolutionFile</literal> for each solver configuration.</para>

      <para>Useful for visualizing the best solver configuration.</para>

      <figure>
        <title>Best Score Summary Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestScoreSummary.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="benchmarkReportBestScoreScalabilitySummary">
      <title>Best Score Scalability Summary (Graph)</title>

      <para>Shows the best score per problem scale for each solver configuration.</para>

      <para>Useful for visualizing the scalability of each solver configuration.</para>

      <note>
        <para>The problem scale will report <literal>0</literal> if any <literal>@ValueRangeProvider</literal> method
        signature returns ValueRange (instead of <literal>CountableValueRange</literal> or
        <literal>Collection</literal>).</para>
      </note>
    </section>

    <section xml:id="benchmarkReportBestScoreDistributionSummary">
      <title>Best Score Distribution Summary (Graph)</title>

      <para>Shows the best score distribution per <literal>inputSolutionFile</literal> for each solver
      configuration.</para>

      <para>Useful for visualizing the reliability of each solver configuration.</para>

      <figure>
        <title>Best Score Distribution Summary Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestScoreDistributionSummary.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Enable <link linkend="statisticalBenchmarking">statistical benchmarking</link> to use this summary.</para>
    </section>

    <section xml:id="benchmarkReportWinningScoreDifferenceSummary">
      <title>Winning Score Difference Summary (Graph And Table)</title>

      <para>Shows the winning score difference per <literal>inputSolutionFile</literal> for each solver configuration.
      The winning score difference is the score difference with the score of the winning solver configuration for that
      particular <literal>inputSolutionFile</literal>.</para>

      <para>Useful for zooming in on the results of the best score summary.</para>
    </section>

    <section xml:id="benchmarkReportWorstScoreDifferencePercentageSummary">
      <title>Worst Score Difference Percentage (ROI) Summary (Graph and Table)</title>

      <para>Shows the return on investment (ROI) per <literal>inputSolutionFile</literal> for each solver configuration
      if you'd upgrade from the worst solver configuration for that particular
      <literal>inputSolutionFile</literal>.</para>

      <para>Useful for visualizing the return on investment (ROI) to decision makers.</para>
    </section>

    <section xml:id="benchmarkReportAverageCalculationCountSummary">
      <title>Average Calculation Count Summary (Graph and Table)</title>

      <para>Shows the score calculation speed: the average calculation count per second per problem scale for each
      solver configuration.</para>

      <para>Useful for comparing different score calculators and/or score rule implementations (presuming that the
      solver configurations do not differ otherwise). Also useful to measure the scalability cost of an extra
      constraint.</para>
    </section>

    <section xml:id="benchmarkReportTimeSpentSummary">
      <title>Time Spent Summary (Graph And Table)</title>

      <para>Shows the time spent per <literal>inputSolutionFile</literal> for each solver configuration. This is
      pointless if it's benchmarking against a fixed time limit.</para>

      <para>Useful for visualizing the performance of construction heuristics (presuming that no other solver phases are
      configured).</para>
    </section>

    <section xml:id="benchmarkReportTimeSpentScalabilitySummary">
      <title>Time Spent Scalability Summary (Graph)</title>

      <para>Shows the time spent per problem scale for each solver configuration. This is pointless if it's benchmarking
      against a fixed time limit.</para>

      <para>Useful for extrapolating the scalability of construction heuristics (presuming that no other solver phases
      are configured).</para>
    </section>

    <section xml:id="benchmarkReportBestScorePerTimeSpentSummary">
      <title>Best Score Per Time Spent Summary (Graph)</title>

      <para>Shows the best score per time spent for each solver configuration. This is pointless if it's benchmarking
      against a fixed time limit.</para>

      <para>Useful for visualizing trade-off between the best score versus the time spent for construction heuristics
      (presuming that no other solver phases are configured).</para>
    </section>
  </section>

  <section xml:id="benchmarkReportStatisticPerDataset">
    <title>Statistic Per Dataset (Graph And CSV)</title>

    <section xml:id="enableAProblemStatistic">
      <title>Enable A Problem Statistic</title>

      <para>The benchmarker supports outputting problem statistics as graphs and CSV (comma separated values) files to
      the <literal>benchmarkDirectory</literal>. To configure one, add a <literal>problemStatisticType</literal>
      line:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens/solved&lt;/benchmarkDirectory&gt;
  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
      &lt;problemStatisticType&gt;CALCULATE_COUNT_PER_SECOND&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;
    ...
  &lt;/inheritedSolverBenchmark&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <para>Multiple <literal>problemStatisticType</literal> elements are allowed.</para>

      <note>
        <para>These statistic per dataset can slow down the solver noticeably, which affects the benchmark results.
        That's why they are optional and not enabled by default.</para>

        <para>The non-optional summary statistics cannot slow down the solver noticeably.</para>
      </note>

      <para>The following types are supported:</para>
    </section>

    <section xml:id="benchmarkReportBestScoreOverTimeStatistic">
      <title>Best Score Over Time Statistic (Graph And CSV)</title>

      <para>To see how the best score evolves over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Best Score Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestScoreStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <note>
        <para>A time gradient based algorithm (such as Simulated Annealing) will have a different statistic if it's run
        with a different time limit configuration. That's because this Simulated Annealing implementation automatically
        determines its velocity based on the amount of time that can be spent. On the other hand, for the Tabu Search
        and Late Annealing, what you see is what you'd get.</para>
      </note>

      <para><emphasis role="bold">The best score over time statistic is very useful to detect abnormalities, such as a
      potential <link linkend="scoreTrap">score trap</link> which gets the solver temporarily stuck in a local
      optima.</emphasis></para>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/letTheBestScoreStatisticGuideYou.png"/>
        </imageobject>
      </mediaobject>
    </section>

    <section xml:id="benchmarkReportStepScoreOverTimeStatistic">
      <title>Step Score Over Time Statistic (Graph And CSV)</title>

      <para>To see how the step score evolves over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;STEP_SCORE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Step Score Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/stepScoreStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Compare the step score statistic with the best score statistic (especially on parts for which the best score
      flatlines). If it hits a local optima, the solver should take deteriorating steps to escape it. But it shouldn't
      deteriorate too much either.</para>

      <warning>
        <para>The step score statistic has been seen to slow down the solver noticeably due to GC stress, especially for
        fast stepping algorithms (such as Simulated Annealing and Late Acceptance).</para>
      </warning>
    </section>

    <section xml:id="benchmarkReportCalculateCountPerSecondStatistic">
      <title>Calculate Count Per Second Statistic (Graph And CSV)</title>

      <para>To see how fast the scores are calculated, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;CALCULATE_COUNT_PER_SECOND&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Calculate Count Per Second Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/calculateCountPerSecondStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <note>
        <para>The initial high calculate count is typical during solution initialization: it's far easier to calculate
        the score of a solution if only a handful planning entities have been initialized, than when all the planning
        entities are initialized.</para>

        <para>After those few seconds of initialization, the calculate count is relatively stable, apart from an
        occasional stop-the-world garbage collector disruption.</para>
      </note>
    </section>

    <section xml:id="benchmarkReportBestSolutionMutationOverTimeStatistic">
      <title>Best Solution Mutation Over Time Statistic (Graph And CSV)</title>

      <para>To see how much each new best solution differs from the <emphasis>previous best solution</emphasis>, by
      counting the number of planning variables which have a different value (not including the variables that have
      changed multiple times but still end up with the same value), add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;BEST_SOLUTION_MUTATION&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Best Solution Mutation Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/bestSolutionMutationStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Use Tabu Search - an algorithm that behaves like a human - to get an estimation on how difficult it would be
      for a human to improve the previous best solution to that new best solution.</para>
    </section>

    <section xml:id="benchmarkReportMoveCountPerStepStatistic">
      <title>Move Count Per Step Statistic (Graph And CSV)</title>

      <para>To see how the selected and accepted move count per step evolves over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;MOVE_COUNT_PER_STEP&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Move Count Per Step Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/moveCountPerStepStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <warning>
        <para>This statistic has been seen to slow down the solver noticeably due to GC stress, especially for fast
        stepping algorithms (such as Simulated Annealing and Late Acceptance).</para>
      </warning>
    </section>

    <section xml:id="benchmarkReportMemoryUseStatistic">
      <title>Memory Use Statistic (Graph And CSV)</title>

      <para>To see how much memory is used, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;MEMORY_USE&lt;/problemStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Memory Use Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/memoryUseStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <warning>
        <para>The memory use statistic has been seen to affect the solver noticeably.</para>
      </warning>
    </section>
  </section>

  <section xml:id="benchmarkReportStatisticPerSingleBenchmark">
    <title>Statistic Per Single Benchmark (Graph And CSV)</title>

    <section xml:id="enableASingleStatistic">
      <title>Enable A Single Statistic</title>

      <para>A single statistic is a statics for 1 dataset for 1 solver configuration. Unlike a problem statistic, it
      does not aggregate over solver configurations.</para>

      <para>The benchmarker supports outputting single statistics as graphs and CSV (comma separated values) files to
      the <literal>benchmarkDirectory</literal>. To configure one, add a <literal>singleStatisticType</literal>
      line:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  &lt;benchmarkDirectory&gt;local/data/nqueens/solved&lt;/benchmarkDirectory&gt;
  &lt;inheritedSolverBenchmark&gt;
    &lt;problemBenchmarks&gt;
      ...
      &lt;problemStatisticType&gt;...&lt;/problemStatisticType&gt;
      &lt;singleStatisticType&gt;PICKED_MOVE_TYPE_BEST_SCORE_DIFF&lt;/singleStatisticType&gt;
    &lt;/problemBenchmarks&gt;
    ...
  &lt;/inheritedSolverBenchmark&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <para>Multiple <literal>singleStatisticType</literal> elements are allowed.</para>

      <note>
        <para>These statistic per single benchmark can slow down the solver noticeably, which affects the benchmark
        results. That's why they are optional and not enabled by default.</para>
      </note>

      <para>The following types are supported:</para>
    </section>

    <section xml:id="benchmarkReportConstraintMatchTotalBestScoreOverTimeStatistic">
      <title>Constraint Match Total Best Score Over Time Statistic (Graph And CSV)</title>

      <para>To see which constraints are matched in the best score (and how much) over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;singleStatisticType&gt;CONSTRAINT_MATCH_TOTAL_BEST_SCORE&lt;/singleStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Constraint Match Total Best Score Diff Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/constraintMatchTotalBestScoreStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Requires the score calculation to support <link linkend="explainingTheScore">constraint matches</link>.
      <link linkend="droolsScoreCalculation">Drools score calculation</link> supports constraint matches automatically,
      but <link linkend="incrementalJavaScoreCalculation">incremental Java score calculation</link> requires more
      work.</para>

      <warning>
        <para>The constraint match total statistics has been seen to affect the solver noticeably.</para>
      </warning>
    </section>

    <section xml:id="benchmarkReportConstraintMatchTotalStepScoreOverTimeStatistic">
      <title>Constraint Match Total Step Score Over Time Statistic (Graph And CSV)</title>

      <para>To see which constraints are matched in the step score (and how much) over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;singleStatisticType&gt;CONSTRAINT_MATCH_TOTAL_STEP_SCORE&lt;/singleStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Constraint Match Total Step Score Diff Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/constraintMatchTotalStepScoreStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Requires the score calculation to support <link linkend="explainingTheScore">constraint matches</link>.
      <link linkend="droolsScoreCalculation">Drools score calculation</link> supports constraint matches automatically,
      but <link linkend="incrementalJavaScoreCalculation">incremental Java score calculation</link> requires more
      work.</para>

      <warning>
        <para>The constraint match total statistics has been seen to affect the solver noticeably.</para>
      </warning>
    </section>

    <section xml:id="benchmarkReportPickedMoveTypeBestScoreDiffOverTimeStatistic">
      <title>Picked Move Type Best Score Diff Over Time Statistic (Graph And CSV)</title>

      <para>To see which move types improve the best score (and how much) over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;singleStatisticType&gt;PICKED_MOVE_TYPE_BEST_SCORE_DIFF&lt;/singleStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Picked Move Type Best Score Diff Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/pickedMoveTypeBestScoreDiffStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section xml:id="benchmarkReportPickedMoveTypeStepScoreDiffOverTimeStatistic">
      <title>Picked Move Type Step Score Diff Over Time Statistic (Graph And CSV)</title>

      <para>To see how much each winning step affects the step score over time, add:</para>

      <programlisting language="xml">    &lt;problemBenchmarks&gt;
      ...
      &lt;singleStatisticType&gt;PICKED_MOVE_TYPE_STEP_SCORE_DIFF&lt;/singleStatisticType&gt;
    &lt;/problemBenchmarks&gt;</programlisting>

      <figure>
        <title>Picked Move Type Step Score Diff Over Time Statistic</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/pickedMoveTypeStepScoreDiffStatistic.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

  <section xml:id="advancedBenchmarking">
    <title>Advanced Benchmarking</title>

    <section xml:id="benchmarkingPerformanceTricks">
      <title>Benchmarking Performance Tricks</title>

      <section xml:id="parallelBenchmarkingOnMultipleThreads">
        <title>Parallel Benchmarking On Multiple Threads</title>

        <para>If you have multiple processors available on your computer, you can run multiple benchmarks in parallel on
        multiple threads to get your benchmarks results faster:</para>

        <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...
  &lt;parallelBenchmarkCount&gt;AUTO&lt;/parallelBenchmarkCount&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

        <warning>
          <para>Running too many benchmarks in parallel will affect the results of benchmarks negatively. Leave some
          processors unused for garbage collection and other processes.</para>

          <para>We tweak <literal>parallelBenchmarkCount</literal> <literal>AUTO</literal> to maximize the reliability
          and efficiency of the benchmark results.</para>
        </warning>

        <para>The following <literal>parallelBenchmarkCount</literal>s are supported:</para>

        <itemizedlist>
          <listitem>
            <para><literal>1</literal> (default): Run all benchmarks sequentially.</para>
          </listitem>

          <listitem>
            <para><literal>AUTO</literal>: Let Planner decide how many benchmarks to run in parallel. This formula is
            based on experience. It's recommended to prefer this over the other parallel enabling options.</para>
          </listitem>

          <listitem>
            <para>Static number: The number of benchmarks to run in parallel.</para>

            <programlisting language="xml">&lt;parallelBenchmarkCount&gt;2&lt;/parallelBenchmarkCount&gt;</programlisting>
          </listitem>

          <listitem>
            <para>JavaScript formula: Formula for the number of benchmarks to run in parallel. It can use the variable
            <literal>availableProcessorCount</literal>. For example:</para>

            <programlisting language="xml">&lt;parallelBenchmarkCount&gt;(availableProcessorCount / 2) + 1&lt;/parallelBenchmarkCount&gt;</programlisting>
          </listitem>
        </itemizedlist>

        <note>
          <para>The <literal>parallelBenchmarkCount</literal> is always limited to the number of available processors.
          If it's higher, it will be automatically decreased.</para>
        </note>

        <note>
          <para>If you have a computer with slow or unreliable cooling, increasing the
          <literal>parallelBenchmarkCount</literal> above 1 (even on <literal>AUTO</literal>) may overheat your
          CPU.</para>

          <para>The <literal>sensors</literal> command can help you detect if this is the case. It is available in the
          package <literal>lm_sensors</literal> or <literal>lm-sensors</literal> in most Linux distributions. There are
          several freeware tools available for Windows too.</para>
        </note>

        <note>
          <para>In the future, we will also support multi-JVM benchmarking. This feature is independent of <link
          xlink:href="https://issues.jboss.org/browse/PLANNER-76">multi-threaded solving</link> or multi-JVM
          solving.</para>
        </note>
      </section>
    </section>

    <section xml:id="statisticalBenchmarking">
      <title>Statistical Benchmarking</title>

      <para>To minimize the influence of your environment and the Random Number Generator on the benchmark results,
      configure the number of times each single benchmark run is repeated. The results of those runs are statistically
      aggregated. Each individual result is also visible in the report, as well as plotted in <link
      linkend="benchmarkReportBestScoreDistributionSummary">the best score distribution summary</link>.</para>

      <para>Just add a <literal>&lt;subSingleCount&gt;</literal> element to an <link
      linkend="inheritedSolverBenchmark"><literal>&lt;inheritedSolverBenchmark&gt;</literal></link> element or in a
      <literal>&lt;solverBenchmark&gt;</literal> element:</para>

      <programlisting language="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;plannerBenchmark&gt;
  ...
  &lt;inheritedSolverBenchmark&gt;
    ...
    &lt;solver&gt;
      ...
    &lt;/solver&gt;
    &lt;subSingleCount&gt;10&lt;subSingleCount&gt;
  &lt;/inheritedSolverBenchmark&gt;
  ...
&lt;/plannerBenchmark&gt;</programlisting>

      <para>The <literal>subSingleCount</literal> defaults to <literal>1</literal> (so no statistical
      benchmarking).</para>

      <note>
        <para>If <literal>subSingleCount</literal> is higher than <literal>1</literal>, the benchmarker will
        automatically use a <emphasis>different</emphasis> <link
        linkend="randomNumberGenerator"><literal>Random</literal> seed</link> for every sub single run, without losing
        reproducibility (for each sub single index) in <link linkend="environmentMode">EnvironmentMode</link>
        <literal>REPRODUCIBLE</literal> and lower.</para>
      </note>
    </section>

    <section xml:id="templateBasedBenchmarking">
      <title>Template Based Benchmarking And Matrix Benchmarking</title>

      <para>Matrix benchmarking is benchmarking a combination of value sets. For example: benchmark 4
      <literal>entityTabuSize</literal> values (<literal>5</literal>, <literal>7</literal>, <literal>11</literal> and
      <literal>13</literal>) combined with 3 <literal>acceptedCountLimit</literal> values (<literal>500</literal>,
      <literal>1000</literal> and <literal>2000</literal>), resulting in 12 solver configurations.</para>

      <para>To reduce the verbosity of such a benchmark configuration, you can use a <link
      xlink:href="http://freemarker.org//">Freemarker</link> template for the benchmark configuration instead:</para>

      <programlisting language="xml">&lt;plannerBenchmark&gt;
  ...

  &lt;inheritedSolverBenchmark&gt;
    ...
  &lt;/inheritedSolverBenchmark&gt;

&lt;#list [5, 7, 11, 13] as entityTabuSize&gt;
&lt;#list [500, 1000, 2000] as acceptedCountLimit&gt;
  &lt;solverBenchmark&gt;
    &lt;name&gt;entityTabuSize ${entityTabuSize} acceptedCountLimit ${acceptedCountLimit}&lt;/name&gt;
    &lt;solver&gt;
      &lt;localSearch&gt;
        &lt;unionMoveSelector&gt;
          &lt;changeMoveSelector/&gt;
          &lt;swapMoveSelector/&gt;
        &lt;/unionMoveSelector&gt;
        &lt;acceptor&gt;
          &lt;entityTabuSize&gt;${entityTabuSize}&lt;/entityTabuSize&gt;
        &lt;/acceptor&gt;
        &lt;forager&gt;
          &lt;acceptedCountLimit&gt;${acceptedCountLimit}&lt;/acceptedCountLimit&gt;
        &lt;/forager&gt;
      &lt;/localSearch&gt;
    &lt;/solver&gt;
  &lt;/solverBenchmark&gt;
&lt;/#list&gt;
&lt;/#list&gt;
&lt;/plannerBenchmark&gt;</programlisting>

      <para>And build it with the class <literal>PlannerBenchmarkFactory</literal>:</para>

      <programlisting language="java">        PlannerBenchmarkFactory plannerBenchmarkFactory = PlannerBenchmarkFactory.createFromFreemarkerXmlResource(
                "org/optaplanner/examples/cloudbalancing/benchmark/cloudBalancingBenchmarkConfigTemplate.xml.ftl");
        PlannerBenchmark plannerBenchmark = plannerBenchmarkFactory.buildPlannerBenchmark();</programlisting>
    </section>

    <section xml:id="benchmarkReportAggregation">
      <title>Benchmark Report Aggregation</title>

      <para>The <literal>BenchmarkAggregator</literal> takes 1 or more existing benchmarks and merges them into new
      benchmark report, without actually running the benchmarks again.</para>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/Chapter-Benchmarking_and_tweaking/benchmarkAggregator.png"/>
        </imageobject>
      </mediaobject>

      <para>This is useful to:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Report on the impact of code changes</emphasis>: Run the same benchmark
          configuration before and after the code changes, then aggregate a report.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Report on the impact of dependency upgrades</emphasis>: Run the same benchmark
          configuration before and after upgrading the dependency, then aggregate a report.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Condense a too verbose report</emphasis>: Select only the interesting solver
          benchmarks from the existing report. This especially useful on template reports to make the graphs
          readable.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Partially rerun a benchmark</emphasis>: Rerun part of an existing report (for
          example only the failed or invalid solvers), then recreate the original intended report with the new
          values.</para>
        </listitem>
      </itemizedlist>

      <para>To use it, provide a <literal>PlannerBenchmarkFactory</literal> to the
      <literal>BenchmarkAggregatorFrame</literal> to display the GUI:</para>

      <programlisting language="java">    public static void main(String[] args) {
        PlannerBenchmarkFactory plannerBenchmarkFactory = PlannerBenchmarkFactory.createFromXmlResource(
                "org/optaplanner/examples/nqueens/benchmark/nqueensBenchmarkConfig.xml");
        BenchmarkAggregatorFrame.createAndDisplay(plannerBenchmarkFactory);
    }</programlisting>

      <warning>
        <para>Despite that it uses a benchmark configuration as input, it ignores all elements of that configuration,
        except for the elements <literal>&lt;benchmarkDirectory&gt;</literal> and
        <literal>&lt;benchmarkReport&gt;</literal>.</para>
      </warning>

      <para>In the GUI, select the interesting benchmarks and click the button to generate the report.</para>

      <note>
        <para>All the input reports which are being merged should have been generated with the same Planner version
        (excluding hotfix differences) as the <literal>BenchmarkAggregator</literal>. Using reports from different
        Planner major or minor versions are not guaranteed to succeed and deliver correct information, because the
        benchmark report data structure often changes.</para>
      </note>
    </section>
  </section>
</chapter>
